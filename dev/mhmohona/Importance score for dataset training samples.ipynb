{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview: \n",
    "\n",
    "This notebook is created to do tasks given in issue no #8: [Importance score for dataset training samples](https://github.com/mozilla/PRESC/issues/8)\n",
    "\n",
    "# Task:\n",
    "- Implement a way to assess the importance of an inidividual training datapoint to the performance of the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  limit_bal  sex  education  marriage  age  pay_0  pay_2  pay_3  pay_4  \\\n",
      "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
      "1   2     120000    2          2         2   26     -1      2      0      0   \n",
      "2   3      90000    2          2         2   34      0      0      0      0   \n",
      "3   4      50000    2          2         1   37      0      0      0      0   \n",
      "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
      "\n",
      "   ...  bill_amt4  bill_amt5  bill_amt6  pay_amt1  pay_amt2  pay_amt3  \\\n",
      "0  ...          0          0          0         0       689         0   \n",
      "1  ...       3272       3455       3261         0      1000      1000   \n",
      "2  ...      14331      14948      15549      1518      1500      1000   \n",
      "3  ...      28314      28959      29547      2000      2019      1200   \n",
      "4  ...      20940      19146      19131      2000     36681     10000   \n",
      "\n",
      "   pay_amt4  pay_amt5  pay_amt6  defaulted  \n",
      "0         0         0         0          1  \n",
      "1      1000         0      2000          1  \n",
      "2      1000      1000      5000          0  \n",
      "3      1100      1069      1000          0  \n",
      "4      9000       689       679          0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Amount of samples: 30000\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../datasets/defaults.csv\")\n",
    "print(dataset.head())\n",
    "print(f\"Amount of samples: {dataset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "\n",
    "SVMs are guaranteed to converge at some point. However, when data is unscaled this can take some time. In this step, some of the features are rescaled to be within the range of [-1, 1]. In this step the id column is also dropped from the dataset since it doesn't provide any relevant information for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   limit_bal  sex  education  marriage       age  pay_0  pay_2  pay_3  pay_4  \\\n",
      "0  -0.979798    2          2         1 -0.896552      2      2     -1     -1   \n",
      "1  -0.777778    2          2         2 -0.827586     -1      2      0      0   \n",
      "2  -0.838384    2          2         2 -0.551724      0      0      0      0   \n",
      "3  -0.919192    2          2         1 -0.448276      0      0      0      0   \n",
      "4  -0.919192    1          2         1  0.241379     -1      0     -1      0   \n",
      "\n",
      "   pay_5  ...  bill_amt4  bill_amt5  bill_amt6  pay_amt1  pay_amt2  pay_amt3  \\\n",
      "0     -2  ...  -0.679724  -0.838704  -0.478043 -1.000000 -0.999182 -1.000000   \n",
      "1      0  ...  -0.673560  -0.831852  -0.473031 -1.000000 -0.998813 -0.997768   \n",
      "2      0  ...  -0.652725  -0.809060  -0.454144 -0.996525 -0.998219 -0.997768   \n",
      "3      0  ...  -0.626382  -0.781274  -0.432630 -0.995421 -0.997603 -0.997322   \n",
      "4      0  ...  -0.640274  -0.800735  -0.448639 -0.995421 -0.956443 -0.977680   \n",
      "\n",
      "   pay_amt4  pay_amt5  pay_amt6  defaulted  \n",
      "0 -1.000000 -1.000000 -1.000000          1  \n",
      "1 -0.996779 -1.000000 -0.992434          1  \n",
      "2 -0.996779 -0.995311 -0.981084          0  \n",
      "3 -0.996457 -0.994987 -0.996217          0  \n",
      "4 -0.971014 -0.996769 -0.997431          0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalize columns 12 to 23. BILL_AMT1 to BILL_AMT6 and PAY_AMT1 to PAY_AMT6\n",
    "norm_columns = [\"limit_bal\", \"pay_amt1\", \"pay_amt2\", \"pay_amt3\", \"pay_amt4\", \"pay_amt5\", \"pay_amt6\",\n",
    "                \"bill_amt1\", \"bill_amt2\", \"bill_amt3\", \"bill_amt4\", \"bill_amt5\", \"bill_amt6\", \"age\"]\n",
    "for c in norm_columns:\n",
    "    max_val = np.max(dataset[c])\n",
    "    min_val = np.min(dataset[c])\n",
    "    \n",
    "    dataset[c] = (dataset[c] - min_val) * 2 / (max_val - min_val) - 1\n",
    "\n",
    "dataset.drop(columns=[\"id\"], inplace=True)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(dataset, valid_per=0.1):\n",
    "    n_samples = dataset.shape[0] # Total number of samples\n",
    "    n_val = int(valid_per * n_samples)\n",
    "    \n",
    "    indices = np.arange(0, n_samples) # Generate a big array with all indices\n",
    "    np.random.shuffle(indices) # Shuffle the array, numpy shuffles inplace\n",
    "    \n",
    "    # Perform the splits\n",
    "    x_train = dataset.iloc[indices[n_val:], :-1].values # Last column is the feature we want to predict\n",
    "    y_train = dataset.iloc[indices[n_val:], -1].values\n",
    "    x_test = dataset.iloc[indices[:n_val], :-1].values\n",
    "    y_test = dataset.iloc[indices[:n_val], -1].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning model\n",
    "\n",
    "**Assuming same class weightings**\n",
    "\n",
    "Now, let's train an SVM classifier to predict weather the credit default will be paid next month. In this step we assume the same weight for both classes, assuming that the dataset is balanced.\n",
    "\n",
    "We can see that the accuracy seems reasonably good, however just looking at this metric masks an important issue that is happening. If take a look at recall values for class 1 we can see that it is VERY low. This means that the classifier is taking an \"easy\" route and just tends to classify most of the samples as class 0 (the dominant class), while underperforming on the other class.\n",
    "\n",
    "This shows that for unbalanced datasets just looking at accuracy isn't enough. We can think of an extreme example were 99% of the samples are of class 0 and just 1% of class 1. In this scenario if the classifier always predicted class 0 for any sample it would achieve an accuracy of 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "\n",
      "#### Linear SVM Results ####\n",
      "Linear SVM Acc: 80.58333333333333 % - Validated on 6000 samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89      4671\n",
      "           1       0.77      0.18      0.29      1329\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6000\n",
      "   macro avg       0.79      0.58      0.59      6000\n",
      "weighted avg       0.80      0.81      0.75      6000\n",
      "\n",
      "\n",
      "#### Polynomial SVM with Degree 3 Results ####\n",
      "Polynomial SVM Acc: 82.35 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      4671\n",
      "           1       0.69      0.36      0.48      1329\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6000\n",
      "   macro avg       0.77      0.66      0.69      6000\n",
      "weighted avg       0.81      0.82      0.80      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting new dataset split...\")\n",
    "# Get a dataset split for training and validation\n",
    "x_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)\n",
    "\n",
    "print(f\"Training on {x_train.shape[0]} samples...\")\n",
    "print(\"\\n#### Linear SVM Results ####\")\n",
    "# Create Linear SVM model\n",
    "lsvm = LinearSVC(max_iter=32000) # If we don't specify anything it assumed all classes have same weight\n",
    "lsvm.fit(x_train, y_train)\n",
    "y_pred = lsvm.predict(x_test)\n",
    "linear_acc = lsvm.score(x_test, y_test)\n",
    "print(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n",
    "# Create Polynomial SVM\n",
    "svm = SVC(gamma='scale', kernel='poly', degree=3)\n",
    "svm.fit(x_train, y_train)\n",
    "poly_acc = svm.score(x_test, y_test)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(f\"Polynomial SVM Acc: {poly_acc*100} %\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning class weightings relative to sample number\n",
    "\n",
    "By initializing the SVM classifiers with the parameter class_weights=\"balanced\" each class is assigned a weight based on its number of samples. The actual calculation is done by `(n_samples / (n_classes * np.bincount(y))`. \n",
    "\n",
    "Let's repeat the test with the same data.\n",
    "\n",
    "We can see that the accuracy took a hit, however the recall for class 1 improved greatly. Nonetheless, the precision for predicting class 1 got much worse. I'm not sure why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 24000 samples...\n",
      "\n",
      "#### Linear SVM Results ####\n",
      "Linear SVM Acc: 69.06666666666666 % - Validated on 6000 samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78      4671\n",
      "           1       0.38      0.65      0.48      1329\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      6000\n",
      "   macro avg       0.63      0.68      0.63      6000\n",
      "weighted avg       0.77      0.69      0.71      6000\n",
      "\n",
      "\n",
      "#### Polynomial SVM with Degree 3 Results ####\n",
      "Polynomial SVM Acc: 77.05 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85      4671\n",
      "           1       0.49      0.60      0.54      1329\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6000\n",
      "   macro avg       0.68      0.71      0.69      6000\n",
      "weighted avg       0.79      0.77      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training on {x_train.shape[0]} samples...\")\n",
    "print(\"\\n#### Linear SVM Results ####\")\n",
    "# Create Linear SVM model\n",
    "lsvm = LinearSVC(max_iter=32000, class_weight=\"balanced\") # Compute weight based on sample count per class\n",
    "lsvm.fit(x_train, y_train)\n",
    "y_pred = lsvm.predict(x_test)\n",
    "linear_acc = lsvm.score(x_test, y_test)\n",
    "print(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n",
    "# Create Polynomial SVM\n",
    "svm = SVC(gamma='scale', kernel='poly', degree=3, \n",
    "          class_weight=\"balanced\") # Compute weight based on sample count per class\n",
    "svm.fit(x_train, y_train)\n",
    "poly_acc = svm.score(x_test, y_test)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(f\"Polynomial SVM Acc: {poly_acc*100} %\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the training dataset and using same class weightings\n",
    "\n",
    "A third scenario we can try here is actually balancing the training dataset and using the same class weightings. This will be done by randomly dropping samples of the dominant class until we have the same number of samples per class.\n",
    "\n",
    "We can observe that there's not much difference from using the class weightings from the previous run. In fact the results are so similar that both ways might be equivalent and the small changes might be just due differences in the dropped samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 10614 samples...\n",
      "\n",
      "#### Linear SVM Results ####\n",
      "Linear SVM Acc: 69.43333333333334 % - Validated on 6000 samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.78      4671\n",
      "           1       0.39      0.65      0.48      1329\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      6000\n",
      "   macro avg       0.63      0.68      0.63      6000\n",
      "weighted avg       0.77      0.69      0.72      6000\n",
      "\n",
      "\n",
      "#### Polynomial SVM with Degree 3 Results ####\n",
      "Polynomial SVM Acc: 77.45 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      4671\n",
      "           1       0.49      0.59      0.54      1329\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6000\n",
      "   macro avg       0.68      0.71      0.69      6000\n",
      "weighted avg       0.79      0.77      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def balanceTrainSet(x_train, y_train):\n",
    "    samples_per_class = np.bincount(y_train) # Count samples per class\n",
    "    dom_class = np.argmax(samples_per_class) # Max class index\n",
    "    min_class = np.argmin(samples_per_class) # Min class index\n",
    "    n_min = samples_per_class[min_class] # Number of samples in min class\n",
    "    \n",
    "    # Get indices for the dominant and the minor class\n",
    "    dom_indices = np.where(y_train == dom_class)[0]\n",
    "    min_indices = np.where(y_train == min_class)[0]\n",
    "    np.random.shuffle(dom_indices) # Shuffle dom_indices\n",
    "    # Contatenate both indices, using only the same number of indices from dom_indices as in min_indices\n",
    "    indices = np.concatenate([min_indices, dom_indices[:n_min]], axis=0)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Build the new training set\n",
    "    new_x_train = x_train[indices]\n",
    "    new_y_train = y_train[indices]\n",
    "    \n",
    "    return new_x_train, new_y_train\n",
    "    \n",
    "bal_x_train, bal_y_train = balanceTrainSet(x_train, y_train)\n",
    "\n",
    "print(f\"Training on {bal_x_train.shape[0]} samples...\")\n",
    "print(\"\\n#### Linear SVM Results ####\")\n",
    "# Create Linear SVM model\n",
    "lsvm = LinearSVC(max_iter=32000) # If we don't specify anything it assumed all classes have same weight\n",
    "lsvm.fit(bal_x_train, bal_y_train)\n",
    "y_pred = lsvm.predict(x_test)\n",
    "linear_acc = lsvm.score(x_test, y_test)\n",
    "print(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n",
    "# Create Polynomial SVM\n",
    "svm = SVC(gamma='scale', kernel='poly', degree=3)\n",
    "svm.fit(bal_x_train, bal_y_train)\n",
    "poly_acc = svm.score(x_test, y_test)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(f\"Polynomial SVM Acc: {poly_acc*100} %\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing 10 Rounds of Cross Validation\n",
    "\n",
    "Now I will perform 10 rounds of cross validation using the SVM method with class weightings relative to the number of samples per class. After 10 rounds the average accuracy and the mean average micro f1-score is presented for both SVM models. Micro F1-score is used because the classes are imbalanced.\n",
    "\n",
    "Observing the results I can conclude that the SVM model with polynomial kernel of degree 3 performs better than the Linear SVM model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Round 1 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 71.55 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 78.16666666666666 %\n",
      "### Round 2 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.95 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 78.01666666666667 %\n",
      "### Round 3 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.68333333333334 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 77.13333333333333 %\n",
      "### Round 4 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.8 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 76.73333333333333 %\n",
      "### Round 5 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.46666666666667 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 77.71666666666667 %\n",
      "### Round 6 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 68.75 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 77.71666666666667 %\n",
      "### Round 7 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.66666666666667 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 76.81666666666666 %\n",
      "### Round 8 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.16666666666667 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 76.43333333333334 %\n",
      "### Round 9 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 70.55 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 77.33333333333333 %\n",
      "### Round 10 ###\n",
      "Getting new dataset split...\n",
      "Training on 24000 samples...\n",
      "Linear SVM Acc: 69.0 % - Validated on 6000 samples\n",
      "Polynomial SVM Acc: 77.33333333333333 %\n",
      "### Finished ###\n",
      "Linear SVM Results:\n",
      "{'0': {'precision': 0.8776285786673423, 'recall': 0.7389078498293515, 'f1-score': 0.8023161551823972, 'support': 4688}, '1': {'precision': 0.40379931807111547, 'recall': 0.631859756097561, 'f1-score': 0.49271916790490344, 'support': 1312}, 'micro avg': {'precision': 0.7155, 'recall': 0.7155, 'f1-score': 0.7154999999999999, 'support': 6000}, 'macro avg': {'precision': 0.6407139483692289, 'recall': 0.6853838029634562, 'f1-score': 0.6475176615436503, 'support': 6000}, 'weighted avg': {'precision': 0.7740179136836342, 'recall': 0.7155, 'f1-score': 0.7346176139643852, 'support': 6000}}\n",
      "\tMean Acc: 69.75833333333333% -- Mean Weighted Avg F1-Score: 71.96592091513516%\n",
      "Polynomial SVM with Degree 3 Results:\n",
      "\tMean Acc: 77.34% -- Mean Weighted Avg F1-Score: 78.0463118279555%\n"
     ]
    }
   ],
   "source": [
    "N_ROUNDS = 10\n",
    "l_reports = [] # Linear SVM reports\n",
    "p_reports = [] # Polynomial SVM reports\n",
    "l_accs = [] # Linear SVM accuracy history\n",
    "p_accs = [] # Polynomial SVM accuracy history\n",
    "for i in range(N_ROUNDS):\n",
    "    print(f\"### Round {i+1} ###\")\n",
    "    print(\"Getting new dataset split...\")\n",
    "    x_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)    \n",
    "    print(f\"Training on {x_train.shape[0]} samples...\")\n",
    "    \n",
    "    # Create a new Linear SVM model\n",
    "    lsvm = LinearSVC(max_iter=32000, class_weight=\"balanced\") # Compute weight based on sample count per class\n",
    "    lsvm.fit(x_train, y_train)\n",
    "    y_pred = lsvm.predict(x_test)\n",
    "    linear_acc = lsvm.score(x_test, y_test)\n",
    "    print(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    l_reports.append(report)\n",
    "    l_accs.append(linear_acc)\n",
    "\n",
    "    # Create Polynomial SVM\n",
    "    svm = SVC(gamma='scale', kernel='poly', degree=3, \n",
    "              class_weight=\"balanced\") # Compute weight based on sample count per class\n",
    "    svm.fit(x_train, y_train)\n",
    "    poly_acc = svm.score(x_test, y_test)\n",
    "    y_pred = svm.predict(x_test)\n",
    "    print(f\"Polynomial SVM Acc: {poly_acc*100} %\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    p_reports.append(report)\n",
    "    p_accs.append(poly_acc)\n",
    "    \n",
    "print(\"### Finished ###\")\n",
    "print(\"Linear SVM Results:\")\n",
    "print(l_reports[0])\n",
    "mean_acc = np.mean(l_accs)\n",
    "mean_f1 = np.mean([r[\"weighted avg\"][\"f1-score\"] for r in l_reports])\n",
    "print(f\"\\tMean Acc: {mean_acc*100}% -- Mean Weighted Avg F1-Score: {mean_f1*100}%\")\n",
    "\n",
    "print(\"Polynomial SVM with Degree 3 Results:\")\n",
    "mean_acc = np.mean(p_accs)\n",
    "mean_f1 = np.mean([r[\"weighted avg\"][\"f1-score\"] for r in p_reports])\n",
    "print(f\"\\tMean Acc: {mean_acc*100}% -- Mean Weighted Avg F1-Score: {mean_f1*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Here, we remove the feature with the least impact on generalization accuracy one-by-one, until there's no remaining features to be removed. The goal is to find whether some features are irrelevant and how much they impact the accuracy of the model. This task will be performed on the same data throughout and using the polynomial of degree 3 model.\n",
    "\n",
    "This reveals a very interesting pattern. Using even one feature we can achieve a very similar performance (even slightly better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train on 4500 samples and validate on 25500 samples.\n",
      "Baseline Acc: 77.08235294117647%\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, pay_2, pay_3, pay_4, pay_5, pay_6, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_2 -- Had the least impact on performance (Acc: 77.64313725490196 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, pay_3, pay_4, pay_5, pay_6, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_3 -- Had the least impact on performance (Acc: 77.8235294117647 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, pay_4, pay_5, pay_6, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_6 -- Had the least impact on performance (Acc: 78.72549019607843 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, pay_4, pay_5, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_4 -- Had the least impact on performance (Acc: 79.09411764705882 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, pay_5, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_5 -- Had the least impact on performance (Acc: 79.92549019607843 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt1 -- Had the least impact on performance (Acc: 80.11372549019607 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt2 -- Had the least impact on performance (Acc: 80.35686274509803 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, marriage, age, pay_0, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature marriage -- Had the least impact on performance (Acc: 80.52549019607844 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, age, pay_0, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature age -- Had the least impact on performance (Acc: 81.03529411764706 %)\n",
      "====================================\n",
      "Remaining features:  limit_bal, sex, education, pay_0, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature limit_bal -- Had the least impact on performance (Acc: 81.24313725490197 %)\n",
      "====================================\n",
      "Remaining features:  sex, education, pay_0, bill_amt3, bill_amt4, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt4 -- Had the least impact on performance (Acc: 81.49411764705883 %)\n",
      "====================================\n",
      "Remaining features:  sex, education, pay_0, bill_amt3, bill_amt5, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt5 -- Had the least impact on performance (Acc: 81.52156862745097 %)\n",
      "====================================\n",
      "Remaining features:  sex, education, pay_0, bill_amt3, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature education -- Had the least impact on performance (Acc: 81.52941176470588 %)\n",
      "====================================\n",
      "Remaining features:  sex, pay_0, bill_amt3, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt3 -- Had the least impact on performance (Acc: 81.96470588235294 %)\n",
      "====================================\n",
      "Remaining features:  sex, pay_0, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature sex -- Had the least impact on performance (Acc: 81.97254901960784 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, bill_amt6, pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_amt3 -- Had the least impact on performance (Acc: 81.97254901960784 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, bill_amt6, pay_amt1, pay_amt2, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_amt2 -- Had the least impact on performance (Acc: 81.9764705882353 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, bill_amt6, pay_amt1, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_amt1 -- Had the least impact on performance (Acc: 81.9764705882353 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, bill_amt6, pay_amt4, pay_amt5, pay_amt6, \n",
      "Removing feature pay_amt4 -- Had the least impact on performance (Acc: 81.9764705882353 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, bill_amt6, pay_amt5, pay_amt6, \n",
      "Removing feature bill_amt6 -- Had the least impact on performance (Acc: 81.96862745098039 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, pay_amt5, pay_amt6, \n",
      "Removing feature pay_amt6 -- Had the least impact on performance (Acc: 81.95294117647059 %)\n",
      "====================================\n",
      "Remaining features:  pay_0, pay_amt5, \n",
      "Removing feature pay_amt5 -- Had the least impact on performance (Acc: 81.96078431372548 %)\n",
      "====================================\n",
      "Last feature is pay_0\n",
      "Accuracy: 81.96078431372548 %\n"
     ]
    }
   ],
   "source": [
    "FEATURE_NAMES = [\"limit_bal\", \"sex\", \"education\", \"marriage\", \"age\", \n",
    "                 \"pay_0\", \"pay_2\", \"pay_3\", \"pay_4\", \"pay_5\", \"pay_6\", \n",
    "                 \"bill_amt1\", \"bill_amt2\", \"bill_amt3\", \"bill_amt4\", \"bill_amt5\", \"bill_amt6\", \n",
    "                 \"pay_amt1\", \"pay_amt2\", \"pay_amt3\", \"pay_amt4\", \"pay_amt5\", \"pay_amt6\"]\n",
    "\n",
    "# Due to computing time constraints use a much smaller training dataset here\n",
    "x_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.85)\n",
    "\n",
    "def trainModel(x_train, y_train, x_test, y_test):\n",
    "    svm = SVC(gamma='scale', kernel='poly', degree=3, \n",
    "              class_weight=\"balanced\")\n",
    "    svm.fit(x_train, y_train)\n",
    "    return svm.score(x_test, y_test)\n",
    "\n",
    "print(f\"Will train on {x_train.shape[0]} samples and validate on {x_test.shape[0]} samples.\")\n",
    "# Train a baseline model for this data, including all the features\n",
    "baseline_acc = trainModel(x_train, y_train, x_test, y_test)\n",
    "print(f\"Baseline Acc: {baseline_acc*100}%\")\n",
    "print(\"====================================\")\n",
    "\n",
    "remaining_features = np.arange(0, x_train.shape[1])\n",
    "for i in range(x_train.shape[1]-1):\n",
    "    feat_names = [FEATURE_NAMES[r] for r in remaining_features]\n",
    "    print(f\"Remaining features: \", end=' ')\n",
    "    [print(feat, end=', ') for feat in feat_names]\n",
    "    print()\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    least_impact_feature = 0\n",
    "    # Find feature with least impact on performance\n",
    "    for c in range(remaining_features.shape[0]):\n",
    "        # Test by removing each of the columns\n",
    "        curr_features = np.delete(remaining_features, c)\n",
    "        part_x_train = x_train[:, curr_features]\n",
    "        part_x_test = x_test[:, curr_features]\n",
    "        \n",
    "        acc = trainModel(part_x_train, y_train, part_x_test, y_test)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            least_impact_feature = c\n",
    "\n",
    "    print(f\"Removing feature {FEATURE_NAMES[remaining_features[least_impact_feature]]} -- Had the least impact on performance (Acc: {best_acc*100} %)\")\n",
    "    remaining_features = np.delete(remaining_features, least_impact_feature)\n",
    "    print(\"====================================\")\n",
    "    \n",
    "print(f\"Last feature is {FEATURE_NAMES[remaining_features[0]]}\")\n",
    "part_x_train = x_train[:, remaining_features]\n",
    "part_x_test = x_test[:, remaining_features]\n",
    "acc = trainModel(part_x_train, y_train, part_x_test, y_test)\n",
    "print(f\"Accuracy: {acc*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering customers with K-Means\n",
    "Here, K-Means is used to cluster the customers based on the features, without taking in account if the default was paid next month. Then, a single cluster is chosen to be considered as the class where the default was paid and the overall accuracy is calculated. The test is repeated for different cluster sizes.\n",
    "\n",
    "We can see that using only 2 clusters yielded the best (although not good) result. This happens probably because as we use a larger number of clusters the data becomes more partitioned, i.e. there's less samples per cluster, so the accuracy takes a drop even though the probability is \"better\" in-cluster. An interesting extension of this is exhaustively searching through all combinations of clusters to find the one that yields the best overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means with 2 clusters:\n",
      "[Cluster 0] - Probability of paid credit in this cluster: 24.94342469197888 %\n",
      "[Cluster 1] - Probability of paid credit in this cluster: 15.126050420168067 %\n",
      "Choosing the single cluster 0 as default_paid yields an overall Accuracy of 16.53333333333333 %\n",
      "\n",
      "K-Means with 4 clusters:\n",
      "[Cluster 0] - Probability of paid credit in this cluster: 15.225845049356865 %\n",
      "[Cluster 1] - Probability of paid credit in this cluster: 49.59016393442623 %\n",
      "[Cluster 2] - Probability of paid credit in this cluster: 15.507411630558723 %\n",
      "[Cluster 3] - Probability of paid credit in this cluster: 66.26506024096386 %\n",
      "Choosing the single cluster 3 as default_paid yields an overall Accuracy of 4.583333333333333 %\n",
      "\n",
      "K-Means with 8 clusters:\n",
      "[Cluster 0] - Probability of paid credit in this cluster: 53.74280230326296 %\n",
      "[Cluster 1] - Probability of paid credit in this cluster: 24.170616113744074 %\n",
      "[Cluster 2] - Probability of paid credit in this cluster: 11.906193625977151 %\n",
      "[Cluster 3] - Probability of paid credit in this cluster: 13.658070678127984 %\n",
      "[Cluster 4] - Probability of paid credit in this cluster: 78.26086956521739 %\n",
      "[Cluster 5] - Probability of paid credit in this cluster: 63.87225548902195 %\n",
      "[Cluster 6] - Probability of paid credit in this cluster: 13.338473400154202 %\n",
      "[Cluster 7] - Probability of paid credit in this cluster: 12.167300380228136 %\n",
      "Choosing the single cluster 4 as default_paid yields an overall Accuracy of 0.3 %\n",
      "\n",
      "K-Means with 16 clusters:\n",
      "[Cluster 0] - Probability of paid credit in this cluster: 53.72340425531915 %\n",
      "[Cluster 1] - Probability of paid credit in this cluster: 14.000000000000002 %\n",
      "[Cluster 2] - Probability of paid credit in this cluster: 13.010204081632654 %\n",
      "[Cluster 3] - Probability of paid credit in this cluster: 72.09302325581395 %\n",
      "[Cluster 4] - Probability of paid credit in this cluster: 42.916666666666664 %\n",
      "[Cluster 5] - Probability of paid credit in this cluster: 17.741935483870968 %\n",
      "[Cluster 6] - Probability of paid credit in this cluster: 12.525667351129362 %\n",
      "[Cluster 7] - Probability of paid credit in this cluster: 24.352331606217618 %\n",
      "[Cluster 8] - Probability of paid credit in this cluster: 56.00000000000001 %\n",
      "[Cluster 9] - Probability of paid credit in this cluster: 12.18568665377176 %\n",
      "[Cluster 10] - Probability of paid credit in this cluster: 44.73684210526316 %\n",
      "[Cluster 11] - Probability of paid credit in this cluster: 25.0 %\n",
      "[Cluster 12] - Probability of paid credit in this cluster: 75.0 %\n",
      "[Cluster 13] - Probability of paid credit in this cluster: 29.47368421052631 %\n",
      "[Cluster 14] - Probability of paid credit in this cluster: 11.067193675889328 %\n",
      "[Cluster 15] - Probability of paid credit in this cluster: 6.734434561626429 %\n",
      "Choosing the single cluster 12 as default_paid yields an overall Accuracy of 0.25 %\n",
      "\n",
      "K-Means with 32 clusters:\n",
      "[Cluster 0] - Probability of paid credit in this cluster: 15.789473684210526 %\n",
      "[Cluster 1] - Probability of paid credit in this cluster: 8.103130755064457 %\n",
      "[Cluster 2] - Probability of paid credit in this cluster: 80.0 %\n",
      "[Cluster 3] - Probability of paid credit in this cluster: 11.940298507462686 %\n",
      "[Cluster 4] - Probability of paid credit in this cluster: 16.363636363636363 %\n",
      "[Cluster 5] - Probability of paid credit in this cluster: 16.43835616438356 %\n",
      "[Cluster 6] - Probability of paid credit in this cluster: 72.6530612244898 %\n",
      "[Cluster 7] - Probability of paid credit in this cluster: 57.42574257425742 %\n",
      "[Cluster 8] - Probability of paid credit in this cluster: 27.659574468085108 %\n",
      "[Cluster 9] - Probability of paid credit in this cluster: 18.367346938775512 %\n",
      "[Cluster 10] - Probability of paid credit in this cluster: 29.67032967032967 %\n",
      "[Cluster 11] - Probability of paid credit in this cluster: 13.750000000000002 %\n",
      "[Cluster 12] - Probability of paid credit in this cluster: 30.0 %\n",
      "[Cluster 13] - Probability of paid credit in this cluster: 13.548387096774196 %\n",
      "[Cluster 14] - Probability of paid credit in this cluster: 91.66666666666666 %\n",
      "[Cluster 15] - Probability of paid credit in this cluster: 37.82383419689119 %\n",
      "[Cluster 16] - Probability of paid credit in this cluster: 14.720812182741117 %\n",
      "[Cluster 17] - Probability of paid credit in this cluster: 13.709677419354838 %\n",
      "[Cluster 18] - Probability of paid credit in this cluster: 11.01871101871102 %\n",
      "[Cluster 19] - Probability of paid credit in this cluster: 13.47305389221557 %\n",
      "[Cluster 20] - Probability of paid credit in this cluster: 5.797101449275362 %\n",
      "[Cluster 21] - Probability of paid credit in this cluster: 21.472392638036812 %\n",
      "[Cluster 22] - Probability of paid credit in this cluster: 65.15151515151516 %\n",
      "[Cluster 23] - Probability of paid credit in this cluster: 12.643678160919542 %\n",
      "[Cluster 24] - Probability of paid credit in this cluster: 45.05494505494506 %\n",
      "[Cluster 25] - Probability of paid credit in this cluster: 10.775862068965516 %\n",
      "[Cluster 26] - Probability of paid credit in this cluster: 29.166666666666668 %\n",
      "[Cluster 27] - Probability of paid credit in this cluster: 58.620689655172406 %\n",
      "[Cluster 28] - Probability of paid credit in this cluster: 42.857142857142854 %\n",
      "[Cluster 29] - Probability of paid credit in this cluster: 60.0 %\n",
      "[Cluster 30] - Probability of paid credit in this cluster: 32.592592592592595 %\n",
      "[Cluster 31] - Probability of paid credit in this cluster: 16.206896551724135 %\n",
      "Choosing the single cluster 14 as default_paid yields an overall Accuracy of 0.18333333333333332 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a dataset split for clustering and validation\n",
    "x_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)    \n",
    "\n",
    "n_clusters = [2, 4, 8, 16, 32]\n",
    "for n in n_clusters:\n",
    "    print(f\"K-Means with {n} clusters:\")\n",
    "    # Create K-Means model\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(x_train) # Fit to training data\n",
    "    # Get clusters from validation data\n",
    "    clustered = kmeans.predict(x_test)\n",
    "    # For each cluster, find the probability defaulting (that the client paid it next month)\n",
    "    # by comparing the number of samples for each class\n",
    "    highest_prob = 0.0\n",
    "    highest_c = 0\n",
    "    overall_acc = 0.0\n",
    "    for c in range(n):\n",
    "        # Retrieve samples that belong to the current cluster\n",
    "        indices = np.where(clustered == c)[0]\n",
    "        samples_in_cluster = [y_test[s] for s in indices]\n",
    "        # How many of those samples are of customers with default paid next month?\n",
    "        proportion = np.bincount(samples_in_cluster)\n",
    "        prob = proportion[1] / np.sum(proportion)\n",
    "        print(f\"[Cluster {c}] - Probability of paid credit in this cluster: {prob*100} %\")\n",
    "        if prob > highest_prob:\n",
    "            highest_c = c\n",
    "            highest_prob = prob\n",
    "            overall_acc = proportion[1] / y_test.shape[0]\n",
    "    \n",
    "    print(f\"Choosing the single cluster {highest_c} as default_paid yields an overall Accuracy of {overall_acc*100} %\")\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
